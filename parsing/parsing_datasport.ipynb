{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Data from [datasport.com](https://www.datasport.com/en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use postman to understand the parameters used by the url request, asked for the exercise.\n",
    "\n",
    "(However, notice that there are equivalent tools for other browser - for instance, for firefox:\n",
    "http://stackoverflow.com/questions/28997326/postman-addons-like-in-firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important modules for this HW\n",
    "import bs4 # doc: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "import requests as rq \n",
    "\n",
    "\n",
    "# previous useful modules\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "form_source = rq.get(\"https://www.datasport.com/en/\")\n",
    "form_soup = bs4.BeautifulSoup(form_source.text, \"html.parser\")\n",
    "# print(form_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the `select` menus of the page, using the `find_all` method of *BeautifulSoup* which allows to search for all tags of a certain type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "selectors = form_soup.find_all('select')\n",
    "print(len(selectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, we can find out what each tag is about by printing the its `name` attribute :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select n°0 : etyp\n",
      "Select n°1 : eventmonth\n",
      "Select n°2 : eventyear\n",
      "Select n°3 : eventlocation\n"
     ]
    }
   ],
   "source": [
    "for num, s in enumerate(selectors):\n",
    "    print(\"Select n°{} : {}\".format(num, s.attrs['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etyp:\n",
      "- All [all]\n",
      "- ---- [all]\n",
      "- Cross-Country-Skiing [Cross-Country-Skiing]\n",
      "- Cycling [Cycling]\n",
      "- Cycling,MTB [Cycling,MTB]\n",
      "- Cycling,Others [Cycling,Others]\n",
      "- Duathlon [Duathlon]\n",
      "- Inline [Inline]\n",
      "- MTB [MTB]\n",
      "- MTB,Cycling [MTB,Cycling]\n",
      "- MTB,Cycling,Others [MTB,Cycling,Others]\n",
      "- MTB,Others [MTB,Others]\n",
      "- MTB,X-Hours [MTB,X-Hours]\n",
      "- Others [Others]\n",
      "- Others,Inline,Running,MTB [Others,Inline,Running,MTB]\n",
      "- Running [Running]\n",
      "- Running,Inline [Running,Inline]\n",
      "- Running,MTB [Running,MTB]\n",
      "- Running,MTB,Others [Running,MTB,Others]\n",
      "- Running,Skiing/Snowboard [Running,Skiing/Snowboard]\n",
      "- Running,Waffenlauf [Running,Waffenlauf]\n",
      "- Running,Walking [Running,Walking]\n",
      "- Running,Walking,MTB [Running,Walking,MTB]\n",
      "- Running,Walking,Others [Running,Walking,Others]\n",
      "- Running,X-Hours [Running,X-Hours]\n",
      "- Skiing/Snowboard [Skiing/Snowboard]\n",
      "- Triathlon [Triathlon]\n",
      "- Triathlon,Duathlon [Triathlon,Duathlon]\n",
      "- Triathlon,Others [Triathlon,Others]\n",
      "- Waffenlauf [Waffenlauf]\n",
      "- Walking [Walking]\n",
      "- X-Hours [X-Hours]\n",
      "- X-Hours,MTB [X-Hours,MTB]\n",
      "eventmonth:\n",
      "- All [all]\n",
      "- ---- [all]\n",
      "- 01 [01]\n",
      "- 02 [02]\n",
      "- 03 [03]\n",
      "- 04 [04]\n",
      "- 05 [05]\n",
      "- 06 [06]\n",
      "- 07 [07]\n",
      "- 08 [08]\n",
      "- 09 [09]\n",
      "- 10 [10]\n",
      "- 11 [11]\n",
      "- 12 [12]\n",
      "eventyear:\n",
      "- All [all]\n",
      "- ---- [all]\n",
      "- 2017 [2017]\n",
      "- 2016 [2016]\n",
      "- 2015 [2015]\n",
      "- 2014 [2014]\n",
      "- 2013 [2013]\n",
      "- 2012 [2012]\n",
      "- 2011 [2011]\n",
      "- 2010 [2010]\n",
      "- 2009 [2009]\n",
      "- 2008 [2008]\n",
      "- 2007 [2007]\n",
      "- 2006 [2006]\n",
      "- 2005 [2005]\n",
      "- 2004 [2004]\n",
      "- 2003 [2003]\n",
      "- 2002 [2002]\n",
      "- 2001 [2001]\n",
      "- 2000 [2000]\n",
      "- 1999 [1999]\n",
      "eventlocation:\n",
      "- All [all]\n",
      "- ---- [all]\n",
      "- Aargau [RCH-AG]\n",
      "- Appenzell Innerhoden [RCH-AI]\n",
      "- Bern [RCH-BE]\n",
      "- Basel-Landschaft [RCH-BL]\n",
      "- Basel-Stadt [RCH-BS]\n",
      "- Fribourg [RCH-FR]\n",
      "- Geneva [RCH-GE]\n",
      "- Glarus [RCH-GL]\n",
      "- Graubünden [RCH-GR]\n",
      "- Lucerne [RCH-LU]\n",
      "- Neuchâtel [RCH-NE]\n",
      "- Nidwalden [RCH-NW]\n",
      "- Obwalden [RCH-OW]\n",
      "- St. Gallen [RCH-SG]\n",
      "- Schaffhausen [RCH-SH]\n",
      "- Solothurn [RCH-SO]\n",
      "- Schwyz [RCH-SZ]\n",
      "- Thurgau [RCH-TG]\n",
      "- Ticino [RCH-TI]\n",
      "- Uri [RCH-UR]\n",
      "- Vaud [RCH-VD]\n",
      "- Valais [RCH-VS]\n",
      "- Zug [RCH-ZG]\n",
      "- Zurich [RCH-ZH]\n",
      "- Vienna [RA-W]\n",
      "- Upper Austria [RA-O]\n",
      "- Salzburg [RA-SA]\n",
      "- Tyrol [RA-T]\n",
      "- Vorarlberg [RA-V]\n",
      "- Styria [RA-ST]\n",
      "- Carinthia [RA-K]\n",
      "- Baden-Württemberg [RD-BW]\n",
      "- Bavaria [RD-BY]\n",
      "- Berlin [RD-BE]\n",
      "- Hamburg [RD-HH]\n",
      "- Hesse [RD-HE]\n",
      "- Lower Saxony [RD-NI]\n",
      "- North Rhine-Westphalia [RD-NW]\n",
      "- Rhineland-Palatinate [RD-RP]\n",
      "- Saarland [RD-SL]\n",
      "- Saxony-Anhalt [RD-ST]\n",
      "- Schleswig-Holstein [RD-SH]\n",
      "- Franche-Comté [RF-I]\n",
      "- Languedoc-Roussillon [RF-K]\n",
      "- Nord - Pas-De-Calais [RF-O]\n",
      "- Poitou-Charentes [RF-T]\n",
      "- Rhône-Alpes [RF-V]\n",
      "- French Islands [RF-W]\n",
      "- Lombardy [RI-LOM]\n",
      "- Piedmont [RI-PMN]\n",
      "- Trentino South Tyrol [RI-TAA]\n",
      "- Veneto [RI-VEN]\n",
      "- Flevoland [RNL-FL]\n",
      "- Friesland [RNL-FR]\n",
      "- Gelderland [RNL-GE]\n",
      "- Groningen [RNL-GR]\n",
      "- Zuid Holland [RNL-ZH]\n",
      "- Blekinge län [RS-K]\n",
      "- Dalarnas län [RS-W]\n",
      "- Gotlands län [RS-I]\n",
      "- Örebro län [RS-T]\n",
      "- Västra Götalands län [RS-O]\n",
      "- ---- [all]\n",
      "- Austria [CA]\n",
      "- Belgium [CB]\n",
      "- Canada [CCAN]\n",
      "- France [CF]\n",
      "- Germany [CD]\n",
      "- Italy [CI]\n",
      "- Liechtenstein [CFL]\n",
      "- Norway [CN]\n",
      "- Switzerland [CCH]\n",
      "- United States [CUSA]\n"
     ]
    }
   ],
   "source": [
    "for s in selectors:\n",
    "    options = s.find_all('option')\n",
    "    options_desc_values = [(o.text, o.attrs['value']) for o in options]\n",
    "    print(s.attrs['name'] + ':')\n",
    "    for (d,v) in options_desc_values:\n",
    "        print(\"- {} [{}]\".format(d,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_url = []\n",
    "list_dates = []\n",
    "list_names = []\n",
    "\n",
    "etyp = 'Running'\n",
    "eventlocation = 'CCH'\n",
    "eventservice = 'all'\n",
    "\n",
    "eventmonth = []\n",
    "for month in range(12):\n",
    "    eventmonth.append(str(month+1).zfill(2))\n",
    "eventyear = []\n",
    "for year in range(1999,2016):\n",
    "    eventyear.append(str(year).zfill(4))\n",
    "\n",
    "yes_date = 0\n",
    "yes_rank = 0\n",
    "yes_name = 0\n",
    "\n",
    "for year in eventyear:\n",
    "    for month in eventmonth:\n",
    "        d = {'etyp': etyp, 'eventlocation': eventlocation, \n",
    "             'eventmonth': month, 'eventservice': eventservice,\n",
    "             'eventyear': year}\n",
    "        post_source = rq.post('https://www.datasport.com/fr/calendrier/',data=d)\n",
    "        form = bs4.BeautifulSoup(post_source.text, \"html.parser\")\n",
    "        #print(form.prettify())\n",
    "        \n",
    "        find_tr = form.find_all('tr')\n",
    "        for tr in find_tr:\n",
    "            if (tr.has_attr('class') and (tr['class'][0]=='even' \n",
    "                                          or tr['class'][0]=='odd')):\n",
    "                all_td = tr.find_all('td')\n",
    "                \n",
    "                find_a = all_td[4].find_all('a')\n",
    "                for a in find_a:\n",
    "                    if (a['href'].startswith('http://services.datasport.com/')\n",
    "                        and not a['href'].endswith('.pdf')):\n",
    "                        list_url.append(a['href'])\n",
    "                        yes_rank = yes_rank + 1\n",
    "                        \n",
    "                if(yes_rank > 0):\n",
    "                    find_a = all_td[1].find_all('a')\n",
    "                    list_names.append(find_a[0].contents[0])\n",
    "                    yes_name = yes_name + 1\n",
    "                \n",
    "                    find_date = all_td[0].find_all('span')\n",
    "                    for date in find_date:\n",
    "                        if (date.has_attr('class') and date['class'][0]==''):\n",
    "                            the_date = date.contents[0]\n",
    "                            if the_date[-1]=='+':\n",
    "                                the_date = the_date[:-1]\n",
    "                            if the_date[-4:]==' bis':\n",
    "                                the_date = the_date[:-4]\n",
    "                            list_dates.append(date.contents[0])\n",
    "                            yes_date = yes_date + 1\n",
    "                \n",
    "                if yes_date != yes_date: #debugging step\n",
    "                    print(str(yes_rank)+' '+str(yes_date)+' '+str(month)+' '+str(year))\n",
    "                    print(list_url[-1])\n",
    "                    print(list_dates[-1])\n",
    "                yes_rank = 0\n",
    "                yes_name = 0\n",
    "                yes_date = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(list_url)==len(list_names))\n",
    "print(len(list_names)==len(list_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_runs_df = pd.DataFrame({ 'Name' : list_names,\n",
    "                    'Date' : list_dates,\n",
    "                    'URL' : list_url })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sam. 27.03.1999</td>\n",
       "      <td>Männedörfler Waldlauf</td>\n",
       "      <td>http://services.datasport.com/1999/zkb/maennedorf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sam. 20.03.1999</td>\n",
       "      <td>Kerzerslauf</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/kerzers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>Luzerner Stadtlauf</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/luzern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>20km de Lausanne</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/km20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>Chäsitzerlouf, Kehrsatz</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/kehrsatz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                     Name  \\\n",
       "0  sam. 27.03.1999    Männedörfler Waldlauf   \n",
       "1  sam. 20.03.1999              Kerzerslauf   \n",
       "2  sam. 24.04.1999       Luzerner Stadtlauf   \n",
       "3  sam. 24.04.1999         20km de Lausanne   \n",
       "4  sam. 24.04.1999  Chäsitzerlouf, Kehrsatz   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://services.datasport.com/1999/zkb/maennedorf  \n",
       "1    http://services.datasport.com/1999/lauf/kerzers  \n",
       "2     http://services.datasport.com/1999/lauf/luzern  \n",
       "3       http://services.datasport.com/1999/lauf/km20  \n",
       "4   http://services.datasport.com/1999/lauf/kehrsatz  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_runs_df.to_csv('links2runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some data\n",
    "\n",
    "In order to get started, we can now start collecting the results from the Lausanne marathone, one of the main early event in Switzerland.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the html of the main page, and __extract the relevant parameters__ to query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "laus_mar_url = 'https://services.datasport.com/2016/lauf/lamara/'\n",
    "result_html = rq.get(laus_mar_url)\n",
    "\n",
    "# use BS to get the classes in which the data is devided:\n",
    "\n",
    "result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "result_font = result_soup.find_all('font')\n",
    "\n",
    "print('number of categories in the main page:', len(result_font))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we look for the ones containing \n",
    "# '*** Overall ***', as they are the most general categories \n",
    "\n",
    "# this is indeed probably a GENERAL KEYWORD, as it's indeed found also in\n",
    "# events in other laungauges, \n",
    "# like https://services.datasport.com/2016/lauf/ascona-locarno-marathon/\n",
    "\n",
    "good_fonts_num = []\n",
    "\n",
    "for n_font, font in enumerate(result_font):\n",
    "    \n",
    "    if 'Overall' in font.findChild().get_text():\n",
    "            \n",
    "        good_fonts_num.append(n_font)\n",
    "        print(font.findChild().get_text())\n",
    "        \n",
    "        \n",
    "good_fonts_num = np.asarray(good_fonts_num)        \n",
    "        \n",
    "#  S***** -.- THERE IS A PROBLEM with the marathon hommes : \n",
    "# they are not in the same 'html shape' .. -.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_fonts_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we have to get all: href=RANG*** b\n",
    "\n",
    "rang_to_query = []\n",
    "\n",
    "for i in range(len(good_fonts_num)-1):\n",
    "        \n",
    "    my_font = result_font[good_fonts_num[i] + 1]\n",
    "    a_tag = my_font.find_all('a')\n",
    "    \n",
    "    for t in a_tag:\n",
    "    \n",
    "        if 'RANG' in t['href']:\n",
    "            \n",
    "            rang_to_query.append(t['href'])\n",
    "        \n",
    "#             print(t['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the datasport.com with the right parameters and finally get the __tables__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = \"https://services.datasport.com/2016/lauf/lamara\"\n",
    "base_url + '/' + rang_to_query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get raw HTML response\n",
    "result_html = rq.get(base_url, params=rang_to_query[0])\n",
    "\n",
    "# Use BeautifulSoup and extract the first (and only) HTML table\n",
    "result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "# result_table = result_soup.find_all('table')[0]\n",
    "\n",
    "# print(result_table.prettify())\n",
    "\n",
    "df_trail = pd.read_html(result_soup.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_trail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******* ******* ******* ******* *******  \n",
    "# OLD CODE \n",
    "# ******* ******* ******* ******* ******* ******* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_html(result_table.decode())[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = df.loc[1]                # use row 2 as column names\n",
    "df = df.drop([0, 1])                  # drop useless first rows\n",
    "df = df.drop([np.nan], axis=1)        # drop useless nan column\n",
    "df.index = df['No Sciper']            # use sciper column as index\n",
    "\n",
    "# Drop some columns\n",
    "df = df.drop(['Orientation Bachelor', 'Orientation Master', 'Filière opt.', 'Type Echange', 'Ecole Echange'], axis=1)\n",
    "\n",
    "# Do some renaming\n",
    "df.index.name = 'sciper'\n",
    "df.columns = ['gender', 'full_name', 'specialization', 'minor', 'status', 'sciper']\n",
    "\n",
    "# Map gender to more standard names\n",
    "dict_gender = {'Monsieur': 'male','Madame': 'female'}\n",
    "df.gender.replace(dict_gender, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tools\n",
    "\n",
    "We can define a helper function which, given a base URL and a dictionary of parameters, will fetch the data and fill a DataFrame with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(base_url, params_dict):\n",
    "    \"\"\"Get data from IS-Academia in a pandas DataFrame\"\"\"\n",
    "    \n",
    "    # Same sequence of operations of above, with a check if the result_table is empty\n",
    "    \n",
    "    result_html = rq.get(base_url,params=params_dict)\n",
    "    result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "    result_table = result_soup.find_all('table')[0]\n",
    "    \n",
    "    if (result_table.text == ''):\n",
    "        # Return empty dataframe\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        # Build a DataFrame containing the data, with SCIPER as index\n",
    "        df = pd.read_html(result_table.decode())[0]\n",
    "        try:\n",
    "            df.columns = df.loc[1]                # use 2nd row as column names\n",
    "            df = df.drop([0, 1])                  # drop useless first rows\n",
    "            df = df.drop([np.nan], axis=1)        # drop useless nan column\n",
    "            df.index = df['No Sciper']            # use sciper column as index\n",
    "        \n",
    "            # Drop some columns\n",
    "            df = df.drop(['Orientation Bachelor', 'Orientation Master', 'Filière opt.', 'Type Echange', 'Ecole Echange'], axis=1)\n",
    "            # Do some renaming\n",
    "            df.index.name = 'sciper'\n",
    "            df.columns = ['gender', 'full_name', 'specialization', 'minor', 'status', 'sciper']\n",
    "            # Map gender to more standard names\n",
    "            dict_gender = {'Monsieur': 'male','Madame': 'female'}\n",
    "            df.gender.replace(dict_gender, inplace=True)\n",
    "        except:\n",
    "            df = pd.DataFrame()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines test this function with hardcoded values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = \"http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.html?\"\n",
    "params_dict = {\n",
    "    'ww_x_GPS': 2021043255,\n",
    "    'ww_i_reportModel': 133685247,\n",
    "    'ww_i_reportModelXsl': 133685270,\n",
    "    'ww_x_UNITE_ACAD': 249847,\n",
    "    'ww_x_PERIODE_ACAD': 355925344,\n",
    "    'ww_x_PERIODE_PEDAGO': 249108,\n",
    "    'ww_x_HIVERETE':2936286\n",
    "}\n",
    "\n",
    "get_data(base_url, params_dict).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's get all the possible values in a cleaner way and keep them in variables that we will use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acad_period = {}\n",
    "level = {}\n",
    "semester = {}\n",
    "acad_unit = {}\n",
    "\n",
    "for s in selectors:\n",
    "    options = s.find_all('option')\n",
    "    options_desc_values = [(o.text, o.attrs['value']) for o in options]\n",
    "    s_name = s.attrs['name']\n",
    "    choices = {d: int(v) for (d,v) in options_desc_values if d!=''}\n",
    "    \n",
    "    if s_name == 'ww_x_PERIODE_ACAD':\n",
    "        acad_period = choices\n",
    "    elif s_name == 'ww_x_PERIODE_PEDAGO':\n",
    "        level = choices\n",
    "    elif s_name == 'ww_x_HIVERETE':\n",
    "        for (d,v) in options_desc_values:\n",
    "            if 'automne' in d:\n",
    "                semester['automne'] = int(v)\n",
    "            elif 'printemps' in d:\n",
    "                semester['printemps'] =int(v)\n",
    "    elif s_name == 'ww_x_UNITE_ACAD':\n",
    "        acad_unit = choices\n",
    "\n",
    "# Example of result\n",
    "acad_period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get bachelor data for every year and store it if it's not empty\n",
    "import os\n",
    "local_dir = '.local-data'\n",
    "try:\n",
    "    os.mkdir(local_dir)\n",
    "except FileExistsError:\n",
    "    # directory exists\n",
    "    print(\"Using existing '\" + local_dir + \"' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fixed values\n",
    "params_dict = {\n",
    "    'ww_x_GPS': -1,\n",
    "    'ww_i_reportModel': 133685247,\n",
    "    'ww_i_reportModelXsl': 133685270,\n",
    "    'ww_x_UNITE_ACAD': acad_unit['Informatique']\n",
    "}\n",
    "\n",
    "# Iterate over all the varying params and keep only data for bachelors\n",
    "for year_key, year_value in acad_period.items():\n",
    "    for level_key, level_value in level.items():\n",
    "        for semester_key, semester_value in semester.items():\n",
    "            if 'bachelor' in level_key.lower():\n",
    "                params_dict['ww_x_PERIODE_ACAD'] = year_value\n",
    "                params_dict['ww_x_PERIODE_PEDAGO'] = level_value\n",
    "                params_dict['ww_x_HIVERETE'] = semester_value\n",
    "                \n",
    "                df = get_data(base_url, params_dict)\n",
    "                if not df.empty:\n",
    "                    # Persist dataframe locally with pickle\n",
    "                    filename = year_key + '-' + level_key.replace(' ', '-').lower() + '-' + semester_key\n",
    "                    df.to_pickle(local_dir + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the previous cell should download 60 files!, as you can check with this command:\n",
    "print(len([name for name in os.listdir(local_dir)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hereby show an example of dataframe laoded from the files previously download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_example = pd.read_pickle(local_dir + '/2007-2008-bachelor-semestre-6-printemps')\n",
    "df_example.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
