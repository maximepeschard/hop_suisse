{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Data from [datasport.com](https://www.datasport.com/en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use postman to understand the parameters used by the url request, asked for the exercise.\n",
    "\n",
    "(However, notice that there are equivalent tools for other browser - for instance, for firefox:\n",
    "http://stackoverflow.com/questions/28997326/postman-addons-like-in-firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important modules for this HW\n",
    "import bs4 # doc: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "import requests as rq \n",
    "import re\n",
    "import time\n",
    "# previous useful modules\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some data\n",
    "\n",
    "In order to get started, we can now start collecting the results from the Lausanne marathone, one of the main early event in Switzerland.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the html of the main page, and __extract the relevant parameters__ to query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the runs main pages\n",
    "Load the csv file links2runs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links2runs=pd.read_csv('links2runs.csv')\n",
    "del links2runs['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links2runs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links2runs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pages links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 test pages.\n",
    "Change only base_url to decide which page to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "laus_mar_url = 'https://services.datasport.com/2016/lauf/lamara/'\n",
    "fri_half_url = 'https://services.datasport.com/2013/lauf/semi-marathon-fribourg/'\n",
    "german_mar_url='https://services.datasport.com/2014/lauf/grmarathon/'\n",
    "kapoag_url='https://services.datasport.com/2013/lauf/kapoag/'\n",
    "laufen_url='https://services.datasport.com/2010/lauf/laufen/'\n",
    "sommer_url='https://services.datasport.com/2014/lauf/sommer-gommer/'\n",
    "emme_url='https://services.datasport.com/2010/lauf/emme/'\n",
    "biel_url='https://services.datasport.com/2009/lauf/bielercross/'\n",
    "lugano_url='https://services.datasport.com/2010/lauf/stralugano/'\n",
    "# PARSED PAGE\n",
    "base_url=laus_mar_url\n",
    "\n",
    "result_html = rq.get(base_url)\n",
    "\n",
    "# use BS to get the classes in which the data is devided:\n",
    "\n",
    "result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "result_font = result_soup.find_all('font')\n",
    "\n",
    "print('number of categories in the main page:', len(result_font))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we look for the classements par ordre alphabetique\n",
    "\n",
    "# FOR THIS IT DOES NOT WORK - category to be got from the category field, not from the pace \n",
    "# https://services.datasport.com/2016/lauf/ascona-locarno-marathon/\n",
    "# https://services.datasport.com/2010/lauf/emme/alfaa.htm\n",
    "\n",
    "def get_links(base_url):\n",
    "    result_html = rq.get(base_url)\n",
    "    result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "    result_font = result_soup.find_all('font')\n",
    "\n",
    "    \n",
    "    links=[] # It contains all the tables to be parsed\n",
    "    for n_font, font in enumerate(result_font):\n",
    "        if font.get('size')=='3':\n",
    "            links_to_process=font.findAll('a')\n",
    "            alfa_found=False\n",
    "            for link in links_to_process:\n",
    "                link=str(link)\n",
    "                try:\n",
    "                    link=link.split('\"')[1]\n",
    "                    if link[:4]=='ALFA':\n",
    "                        links.append(base_url+'/'+link)\n",
    "                        alfa_found=True\n",
    "                    elif alfa_found:\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "            break\n",
    "    print('links found:', len(links))\n",
    "\n",
    "    return links\n",
    "\n",
    "links=get_links(base_url)\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the datasport.com with the right parameters and finally get the __tables__\n",
    "\n",
    "A lot of checks are done to check if the table structure is standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table format check\n",
    "The table has to contains these fields according to the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are more fields than that. These are the only the ones that matters\n",
    "# Important to automatically check if some tables are differently structured\n",
    "# Impossible to manually check all the tables for all the games.\n",
    "header_fields_french=[['catégorie'],['rang'],['nom et prénom','nom/lieu','nom','Name und Vorname','Name/Ort'],['an','Jg'],['équipe/lieu','lieu','pays/lieu','Land/Ort','Team/Ortschaft'],['équipe'],['pénalité'],['temps'],['retard']]\n",
    "optional_french=['pénalité','équipe','retard']\n",
    "first_excluded_field_french='doss'\n",
    "last_field_french=['moyenne','Ø/km','km/h']\n",
    "header_fields_german=[['Kategorie'],['Rang'],['Name und Vorname','Name/Ort','Name','@Namealle'],['Jg','jg'],['Team/Ortschaft','Land/Ort','Team','Ortschaft','Ort'],['S Start','Wohnort'],['Team'],['Nat'],['Verband'],['Schweizermeist.'],['Zeit'],['Rückstand']]\n",
    "optional_german=['Team','Rückstand','S Start','Nat','Wohnort','Verband','Schweizermeist.']\n",
    "first_excluded_field_german='Stnr'\n",
    "last_field_german=['Schnitt','Ø/km','km/h']\n",
    "header_fields_italian=[['categoria'],['posto'],['Nome/Località','nome/località','nome'],['anno','Anno','an'],['squadra/località','squadra/luogo','Squadra/Località','località'],['squadra'],['tempo'],['ritardo']]\n",
    "optional_italian=['nazione','squadra','ritardo']\n",
    "first_excluded_field_italian='pett'\n",
    "last_field_italian=['media','Ø/km','km/h']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse_time() will be used both to parse the time fields and to check if a field is a time field or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_time(time,check_only=False,split=True):\n",
    "    ''' Return a parsing of the time\n",
    "    '''\n",
    "    if split:\n",
    "        time=time.split(' ')[0]\n",
    "    if time.count(',')==0 and not check_only:\n",
    "        raise()\n",
    "    time=re.split(\"[:.,]+\",time)\n",
    "    while len(time)<4:\n",
    "        time=[0]+time\n",
    "    hours,minutes,seconds,mseconds=[float(x) for x in time]\n",
    "    \n",
    "    if not check_only:\n",
    "        return (hours,minutes,seconds,mseconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process_legend() is a function to check if the table has a standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_legend(legend):\n",
    "    ''' Check if the legend is in a compatible format and find the language of the legend\n",
    "    @return language, if pace is available\n",
    "    \n",
    "    The pace is necessary to get the distance of the run if it is not available in the description.\n",
    "    '''\n",
    "    legend_start=str(legend)\n",
    "    legend=str(legend).split('\\n')[0]\n",
    "    if 'TdCN' in legend or 'Waffenlauf' in legend or 'DATASPORT' in legend:\n",
    "        legend=legend_start.split('\\n')[1]\n",
    "    legend=legend.split('¦')[0]\n",
    "    legend=re.sub('<[^>]+>', ' ', legend)\n",
    "    legend=legend.lstrip()\n",
    "    # check language\n",
    "    if legend.startswith(header_fields_french[0][0]):\n",
    "        language='French'\n",
    "        header_fields=header_fields_french\n",
    "        first_excluded=first_excluded_field_french\n",
    "        optional=optional_french\n",
    "        last_field=last_field_french\n",
    "    elif legend.startswith(header_fields_german[0][0]):\n",
    "        language='German'\n",
    "        header_fields=header_fields_german\n",
    "        first_excluded=first_excluded_field_german\n",
    "        optional=optional_german\n",
    "        last_field=last_field_german\n",
    "    elif legend.startswith(header_fields_italian[0][0]):\n",
    "        language='Italian'\n",
    "        header_fields=header_fields_italian\n",
    "        first_excluded=first_excluded_field_italian\n",
    "        optional=optional_italian\n",
    "        last_field=last_field_italian\n",
    "    else:\n",
    "        print(legend)\n",
    "        raise('Error, problems in language detection')\n",
    "        return '',False,True\n",
    "    \n",
    "    # Check if all words are present\n",
    "    for words in header_fields:\n",
    "        found=False\n",
    "        for word in words:\n",
    "            if legend.startswith(word):\n",
    "                legend=legend.split(word)[1]\n",
    "                legend=legend.lstrip()\n",
    "                found=True\n",
    "                break\n",
    "            \n",
    "        if found==False:\n",
    "            if words[0] in optional:\n",
    "                pass\n",
    "            else:\n",
    "                print(words)\n",
    "                print(legend)\n",
    "                raise('Error, word not known')\n",
    "                return '',False,True\n",
    "    legend_splitted=legend.split(' ')\n",
    "    legend_first_excluded=legend_splitted[0]\n",
    "    if legend_first_excluded != first_excluded:\n",
    "        print(legend_first_excluded)\n",
    "        print(legend)\n",
    "        raise('First excluded element not good')\n",
    "    legend_splitted=[x.lstrip() for x in legend_splitted]\n",
    "    legend_splitted=[x for x in legend_splitted if x!='' ]\n",
    "    last=legend_splitted[-1]\n",
    "\n",
    "    for word in last_field:\n",
    "        if last.startswith(word):\n",
    "            if word=='km/h':\n",
    "                return language,word\n",
    "            return language,True\n",
    "    \n",
    "    \n",
    "    \n",
    "    return language,False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fields* - standard fields for each language:\n",
    "1. catégorie (0)\n",
    "2. rang (1) (CAN BE MERGED WITH NOM)\n",
    "3. nom (2) (CAN BE MERGED WITH RANG)\n",
    "4. an (3)\n",
    "5. lieu (3)\n",
    "6. équipe  (4) (MAYBE MISSING)\n",
    "7. pénalité (5) (NOT ALWAYS PRESENT)\n",
    "8. temps (6)\n",
    "9. retard (7)\n",
    "\n",
    "*Only* 1,2,3,4,5,8,9 are parsed!!\n",
    "After 5, it checks if the other fields are a time field. If they are not, they are not used.\n",
    "If more than 2 times are found an error is raised.\n",
    "If 1 time is found it is supposed that it is the final time, not the delay.\n",
    "If 0 times are found the player is not used and it is printed\n",
    "\n",
    "The presence of these fields is automatically checked in process_legend(). They have to be in this order.\n",
    "If they are not, an error is raised.\n",
    "Other possible problems:\n",
    "1. temps and retard should be formatted in a way parsable by parse_time()\n",
    "2. Also the other fields should be formatted in the same way as Lausanne Marathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing of category/sex/length\n",
    "We are not interested in the specific category of the race. It will be deduced by the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are strongly interested in:\n",
    "1. Sex\n",
    "2. Length of the race\n",
    "\n",
    "These informations are not easily parsable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE VERIFIED\n",
    "\n",
    "It seems that *sex* is always included in some way in category: here are the words in the second part of the category string that contains the sex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't parse if it ends with 'W', it can be a walking and it makes confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "men_category=['Hommes','Herren','Boys','Hom','Gar']\n",
    "men_category_starting_ending_word=['H','M']\n",
    "women_category=['Femmes','Damen','Girls','Dam','Fam','Fille']\n",
    "women_category_starting_ending_word=['D','F']\n",
    "women_category_only_starting_word=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_or_majuscule(letter):\n",
    "    return (letter.isdigit() or letter.isupper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields parsing\n",
    "The fields are parsed in process_fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find sex of people: https://github.com/jacobkap/nameSexRace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_category(category):  \n",
    "    split=category.split('-')\n",
    "    if len(split)==2:\n",
    "        first,second=split\n",
    "    elif len(split)==1:\n",
    "        second=split[0]\n",
    "        first=False\n",
    "    else: \n",
    "#         print('Category not expected:',category)\n",
    "        first=category[0]\n",
    "        second='-'.join(category[1:])\n",
    "#         raise('Category not expected')\n",
    "    # Category retrieval\n",
    "    try:\n",
    "        float(first)\n",
    "    except:\n",
    "        first=False\n",
    "    # We return always the category - they will be processed later\n",
    "    first=category\n",
    "    \n",
    "    # Sex retrieval\n",
    "    sex=False\n",
    "    for word in men_category:\n",
    "        if word in second:\n",
    "            sex='M'\n",
    "            break\n",
    "    for word in men_category_starting_ending_word:\n",
    "        if (second.startswith(word) and number_or_majuscule(second[len(word):])) or second.endswith(word):\n",
    "            sex='M'\n",
    "            break\n",
    "    for word in women_category:\n",
    "        if word in second:\n",
    "            if sex=='M':\n",
    "#                 print('Double sex detected:', category)\n",
    "                sex=False\n",
    "                return first,sex\n",
    "#                 raise('Double sex detected')\n",
    "            sex='F'\n",
    "            break\n",
    "    for word in women_category_starting_ending_word:\n",
    "        if (second.startswith(word) and number_or_majuscule(second[len(word):])): \n",
    "            if sex=='M':\n",
    "#                 print('Double sex detected:', category)\n",
    "                sex=False\n",
    "                return first,sex\n",
    "#                 raise('Double sex detected')\n",
    "            sex='F'\n",
    "            break\n",
    "    for word in women_category_only_starting_word:\n",
    "        if second.startswith(word): \n",
    "            if sex=='M':\n",
    "#                 print('Double sex detected:', category)\n",
    "                sex=False\n",
    "                return first,sex\n",
    "#                 raise('Double sex detected')\n",
    "            sex='F'\n",
    "            break\n",
    "    return first,sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields that can be found\n",
    " - Category, present in all the dataset and processed by process_category()\n",
    " - rang, nom, an, pays/lieu. They can be splitted 1,1,2 or 2,2 or 4 typically\n",
    " - temps, easy to process in general\n",
    " - retard, easy to process in general if present\n",
    " - pace, easy to process in general if present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "1. Process category\n",
    "2. Get the temps field\n",
    "3. Process all the fields between category and temps\n",
    "4. Process retard and pace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_fields(runner_splitted,pace):\n",
    "    ''' @ paramethers\n",
    "            runner_splitted is a list of fields. It is created by the for loop in the Parsing section.\n",
    "                It is not well formatted. Some fields can be merged together. Check hypothesis.\n",
    "        @ returns\n",
    "            the list of fields that will be directly imported in the database\n",
    "    '''\n",
    "    fields_processed=[]\n",
    "    # The first element is the category - process it\n",
    "    fields_processed+=process_category(runner_splitted[0])\n",
    "    # Check if splitting second element\n",
    "    try:\n",
    "        splitted=runner_splitted[1].split('.')\n",
    "    except:\n",
    "        print(runner_splitted)\n",
    "        raise()\n",
    "    try:\n",
    "        splitted[0]=int(splitted[0])\n",
    "    except:\n",
    "#         print('Bad rank')\n",
    "        return ['Bad rank']\n",
    "    \n",
    "    if len(splitted)!=1 and splitted[1]!='': #rang and nom are merged\n",
    "        splitted[1]=splitted[1].lstrip()\n",
    "        splitted[1]='.'.join(splitted[1:])\n",
    "        fields_processed+=splitted[:2]\n",
    "        first_to_check=2\n",
    "    else:\n",
    "        fields_processed.append(splitted[0])\n",
    "        fields_processed.append(runner_splitted[2])\n",
    "        first_to_check=3\n",
    "    \n",
    "    if len(fields_processed)!=4:\n",
    "        print('before')\n",
    "        print(fields_processed,runner_splitted)\n",
    "        raise()\n",
    "        \n",
    "\n",
    "    # Check if nom is merged with an-lieu\n",
    "    try:\n",
    "        parse_time(runner_splitted[first_to_check])\n",
    "        splitted_name_an=fields_processed[-1].split(' ')\n",
    "        added_year=False\n",
    "        for i,word in enumerate(splitted_name_an):\n",
    "            try:\n",
    "                int(word)\n",
    "                fields_processed[-1]=fields_processed[-1].split(word)[0]\n",
    "                fields_processed.append(word)\n",
    "                fields_processed.append(' '.join(splitted_name_an[i+1:]))\n",
    "                added_year=True\n",
    "                break\n",
    "            except:\n",
    "                if word=='??' or word=='????':\n",
    "                    print('Added ?? as year:',runner_splitted)\n",
    "                    fields_processed[-1]=fields_processed[-1].split(word)[0]\n",
    "                    fields_processed.append(word)\n",
    "                    fields_processed.append(' '.join(splitted_name_an[i+1:]))\n",
    "                    added_year=True\n",
    "                    break\n",
    "        if not added_year:\n",
    "            runner_splitted.append('---')\n",
    "    except:        \n",
    "        # Split the an-lieu element\n",
    "        try:\n",
    "            fields_processed+=runner_splitted[first_to_check].split(' ',1)\n",
    "        except:\n",
    "            print(fields_processed)\n",
    "            print(runner_splitted)\n",
    "            raise()\n",
    "        first_to_check+=1\n",
    "        if len(fields_processed)<6:\n",
    "            try:\n",
    "                parse_time(runner_splitted[first_to_check])\n",
    "            except:\n",
    "                del fields_processed[4:]\n",
    "                try:\n",
    "                    fields_processed+=runner_splitted[first_to_check].split(' ',1)\n",
    "                except:\n",
    "                    print(fields_processed)\n",
    "                    print(runner_splitted)\n",
    "                    raise()\n",
    "                first_to_check+=1\n",
    "        # Add if they are not present\n",
    "        while len(fields_processed)<6:\n",
    "            fields_processed.append('---')\n",
    "            print('Added an-lieu:',fields_processed,runner_splitted)\n",
    "\n",
    "\n",
    "        # Take only the first element (the year). The second is kept only if it is a time (not encountered yet)\n",
    "        try:\n",
    "            parse_time(fields_processed[-1])\n",
    "            raise('It should not be a date')\n",
    "        except:\n",
    "            pass\n",
    "            #del fields_processed[-1]\n",
    "    if len(fields_processed)!=6:\n",
    "        if runner_splitted[1].split('.')[1]=='':\n",
    "#             print('Missing name:',fields_processed,runner_splitted)\n",
    "            return ['Missing name']\n",
    "        print(fields_processed,runner_splitted)\n",
    "        raise()\n",
    "    \n",
    "        \n",
    "    # Insert all times found after the year (if they are not 2 raise an error)\n",
    "    added_fields=0\n",
    "    for i in range(first_to_check,len(runner_splitted)):\n",
    "        try:\n",
    "            parse_time(runner_splitted[i])\n",
    "            fields_processed.append(runner_splitted[i].split(' ')[0])\n",
    "            added_fields+=1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if added_fields==0:\n",
    "        print('No added fields')\n",
    "        print(runner_splitted[first_to_check:])\n",
    "        return ['No added fields']\n",
    "    if added_fields==1:\n",
    "        fields_processed.append('----')\n",
    "        added_fields=2\n",
    "    if added_fields!=2:\n",
    "        if added_fields!=3 or pace!='km/h':\n",
    "            print('More than 2 added fields:',runner_splitted)\n",
    "        for i in range(2,added_fields):\n",
    "            del fields_processed[-1]\n",
    "#         print(added_fields)\n",
    "#         print(runner_splitted)\n",
    "#         raise('Added fields not equal to 2')\n",
    "    \n",
    "    # Add pace if present\n",
    "    if pace:\n",
    "        try:\n",
    "            parse_time(runner_splitted[-1],check_only=True)\n",
    "            if pace=='km/h':\n",
    "                ms=float(runner_splitted[-1].replace(',','.'))/3.6\n",
    "                sm=1000/ms\n",
    "                minutes=int(sm/60)\n",
    "                sec=int(sm%60)\n",
    "                runner_splitted[-1]=str(minutes)+'.'+str(sec)\n",
    "            fields_processed.append(runner_splitted[-1])\n",
    "        except:\n",
    "#             print(fields_processed)\n",
    "#             print(runner_splitted)\n",
    "            return ['pace not present']\n",
    "            raise('pace not present')\n",
    "    else:\n",
    "        fields_processed.append(False)\n",
    "        \n",
    "    return fields_processed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_fields2(runner_splitted,pace):\n",
    "    ''' @ paramethers\n",
    "            runner_splitted is a list of fields. It is created by the for loop in the Parsing section.\n",
    "                It is not well formatted. Some fields can be merged together. Check hypothesis.\n",
    "        @ returns\n",
    "            the list of fields that will be directly imported in the database\n",
    "    '''\n",
    "    if runner_splitted[0]=='':\n",
    "        del runner_splitted[0]\n",
    "    \n",
    "    fields_processed=[]\n",
    "    # The first element is the category - process it\n",
    "    fields_processed+=process_category(runner_splitted[0])\n",
    "    \n",
    "    # Get the temps field\n",
    "    found=False\n",
    "    for i,field in enumerate(runner_splitted[1:]):\n",
    "        try:\n",
    "            parse_time(field)\n",
    "            found=True\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "    if found==True:\n",
    "        temps_field=i+1 # +1 since the 0 index is runner_splitted=1\n",
    "    else:\n",
    "        return ['Missing time']\n",
    "    \n",
    "    # Parse the field between category and temps\n",
    "    middle_fields=runner_splitted[1:temps_field]\n",
    "    \n",
    "    # Rang\n",
    "    try:\n",
    "        splitted=middle_fields[0].split('.')\n",
    "    except:\n",
    "        print(runner_splitted)\n",
    "        raise()\n",
    "    try:\n",
    "        rang=int(splitted[0])\n",
    "    except:\n",
    "        return ['Bad rank']\n",
    "    fields_processed.append(rang)\n",
    "    if len(splitted)!=1 and splitted[1].lstrip()!='': #rang and nom are merged - put name in middle fields\n",
    "        splitted[1]=splitted[1].lstrip()\n",
    "        middle_fields[0]='.'.join(splitted[1:])\n",
    "    else:\n",
    "        del middle_fields[0]\n",
    "    \n",
    "    # Check name exists\n",
    "    try:\n",
    "        name=middle_fields[0]\n",
    "    except:\n",
    "        return ['Missing name']\n",
    "    \n",
    "    # Get year\n",
    "    middle_field_join=' '.join(middle_fields)\n",
    "    try:\n",
    "        year=[s for s in middle_field_join.split(' ') if s.isdigit() or s=='??' or s=='????' or s=='--' or s=='xxxx'][0]\n",
    "    except:\n",
    "        print(middle_fields)\n",
    "        print(middle_field_join)\n",
    "        raise()\n",
    "    \n",
    "    # Get name and lieu\n",
    "    for i,field in enumerate(middle_fields):\n",
    "        if year in field:\n",
    "            field_first_part=field.split(year)[0]\n",
    "            name=' '.join(middle_fields[:i])+' '+field_first_part\n",
    "            field_last_part=field.split(year)[1]\n",
    "            if field_last_part!='':\n",
    "                lieu=field_last_part\n",
    "            else:\n",
    "                try:\n",
    "                    lieu=middle_fields[i+1]\n",
    "                except:\n",
    "                    lieu='----'\n",
    "            break\n",
    "    fields_processed.append(name)\n",
    "    fields_processed.append(year)\n",
    "    fields_processed.append(lieu)\n",
    "    \n",
    "#     splitted=name.split(' ')\n",
    "#     year_index=len(splitted)\n",
    "#     for i,field in enumerate(splitted): \n",
    "#         # 2 cases of merged items: int detected or ??,???? detected\n",
    "#         # These are the year\n",
    "#         try:\n",
    "#             int(field)\n",
    "#             middle_fields[0]=' '.join(splitted[i:])\n",
    "#             year_index=i\n",
    "#             break\n",
    "#         except:\n",
    "#             if field=='??' or field=='????':\n",
    "#                 middle_fields[0]=' '.join(splitted[i:])\n",
    "#                 year_index=i\n",
    "#                 break\n",
    "#     name=' '.join(splitted[:year_index])\n",
    "#     fields_processed.append(name)\n",
    "#     if name==middle_fields[0]:\n",
    "#         del middle_fields[0]\n",
    "    \n",
    "#     # Year\n",
    "#     while 1:\n",
    "#         if len(middle_fields)==0:\n",
    "#             return ['No year']\n",
    "#         splitted=middle_fields[0].split(' ')\n",
    "#         try:\n",
    "#             year=int(splitted[0])\n",
    "#             break\n",
    "#         except:\n",
    "#             if splitted[0]=='??' or splitted[0]=='????':\n",
    "#                 year=splitted[0]\n",
    "#                 break\n",
    "#             del middle_fields[0]\n",
    "#     fields_processed.append(year)\n",
    "#     if len(splitted)!=1 and splitted[1].lstrip()!='': #year and lieu - put lieu in middle fields\n",
    "#         splitted[1]=splitted[1].lstrip()\n",
    "#         middle_fields[0]=' '.join(splitted[1:])\n",
    "#     else:\n",
    "#         del middle_fields[0]\n",
    "        \n",
    "#     # Lieu\n",
    "#     try:\n",
    "#         lieu=middle_fields[0]\n",
    "#         fields_processed.append(lieu)\n",
    "#     except:\n",
    "#         print('Missing lieu:',runner_splitted)\n",
    "#         fields_processed.append('---')\n",
    "    \n",
    "    # Temps\n",
    "    temps=runner_splitted[temps_field]\n",
    "    fields_processed.append(temps)\n",
    "    try:\n",
    "        parse_time(runner_splitted[temps_field+1])\n",
    "        fields_processed.append(runner_splitted[temps_field+1])\n",
    "    except:\n",
    "        fields_processed.append('---')\n",
    "    \n",
    "    # Pace\n",
    "    if pace:\n",
    "        try:\n",
    "            parse_time(runner_splitted[-1],check_only=True)\n",
    "            if pace=='km/h':\n",
    "                ms=float(runner_splitted[-1].replace(',','.'))/3.6\n",
    "                sm=1000/ms\n",
    "                minutes=int(sm/60)\n",
    "                sec=int(sm%60)\n",
    "                runner_splitted[-1]=str(minutes)+'.'+str(sec)\n",
    "            fields_processed.append(runner_splitted[-1])\n",
    "        except:\n",
    "            return ['pace not present']\n",
    "    else:\n",
    "        fields_processed.append(False)\n",
    "        \n",
    "    return fields_processed\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_parse(runner):\n",
    "    return True\n",
    "    start=runner[:3]\n",
    "    if start=='10-' or start=='21-' or start=='42-':\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i,link in enumerate(['http://services.datasport.com/2002/lauf/biel']):\n",
    "t1=time.time()\n",
    "final_list=[]\n",
    "\n",
    "error_links=[]\n",
    "for i,link in enumerate(links2runs.URL):\n",
    "    if i==87: # Not working - distance not available in any case\n",
    "        continue\n",
    "    if i==120: # https://services.datasport.com/2000/lauf/jungfrau/\n",
    "        continue\n",
    "    if i==176: # https://services.datasport.com/2001/lauf/zuerimeitli/ - no time\n",
    "        continue\n",
    "    if i==257: # https://services.datasport.com/2002/lauf/zuerimeitli/ - no time\n",
    "        continue \n",
    "    if i==267: # https://services.datasport.com/2002/lauf/defi/default.htm - no distance and difficult to parse\n",
    "        continue \n",
    "    if i==269: #https://services.datasport.com/2002/lauf/defi_kids/ - kid race - no time\n",
    "        continue\n",
    "    if i==552: #https://services.datasport.com/2005/lauf/chur/ - year missing\n",
    "        continue\n",
    "#     if i!=264:\n",
    "#         continue\n",
    "    \n",
    "    if i<1000:\n",
    "        continue\n",
    "#     if i==1000:\n",
    "#         break\n",
    "    print(i,link)\n",
    "    links=get_links(link)\n",
    "\n",
    "    added_runner=0\n",
    "    for link in links:\n",
    "        # Get raw HTML response\n",
    "        result_html = rq.get(link)#, params=rang_to_query[0])\n",
    "\n",
    "        # Use BeautifulSoup and extract the first (and only) HTML table\n",
    "        result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "\n",
    "        results=result_soup.findAll('font')  # Search for all fonts\n",
    "        try:\n",
    "#             print(repr(results[0])\n",
    "            if 'DATASPORT Diplom Service für den Schweizer Frauenlauf' in str(results[0]):\n",
    "                while 'Kategorie' not in str(results[0]):\n",
    "                    del results[0]\n",
    "            language,pace=process_legend(results[0])\n",
    "        except:\n",
    "            if i!=396 or i!=592: # https://services.datasport.com/2003/lauf/silvester/ - strange legend, it should work\n",
    "                continue\n",
    "                print('Link not working')\n",
    "            else:\n",
    "                pace=False\n",
    "                language='German'\n",
    "#         print(language,pace)\n",
    "        del results[0]    # This is the legend\n",
    "        for table in results:\n",
    "            if table.get('size')=='2': # If size is 1 it stores the split times, not interesting\n",
    "                # NOT TRUE IN GENERAL !!!!!!!!!!!!!!!!!!!!\n",
    "                try:\n",
    "                    runner_list=str(table).split('\\n')         # Each line is delimited by \\n\n",
    "                except:\n",
    "#                     print('Infinite recursion')\n",
    "                    break\n",
    "                font_found=False\n",
    "                for k,runner in enumerate(runner_list):\n",
    "                    if '<font' in runner:\n",
    "                        if font_found:\n",
    "                            break\n",
    "                        font_found=True\n",
    "                    runner=runner.split('¦')[0] # The part on the right of ¦ is composed by partial times if present\n",
    "                    start_runner=runner[:]\n",
    "                    runner=re.sub('<[^>]+>', ' ', runner) # Remove all text between <>\n",
    "                    runner=re.sub('  +','#@$&',runner)       # Replace all the double or more spaces with &\n",
    "\n",
    "                    runner=runner.replace('\\n','')        # Remove the \\n at the beginning of the line\n",
    "\n",
    "\n",
    "                    runner=runner.replace(' \\r','')       # Remove the \\r at the beginning of the line\n",
    "                    runner=runner.replace('\\r','')       # Remove the \\r at the beginning of the line\n",
    "                    runner=runner.lstrip()                 # The first athlete starts with a space\n",
    "\n",
    "                    # The team can be empty, check:\n",
    "                    start=runner.split('#@$&')[0]\n",
    "                    if do_parse(start):\n",
    "                        runner2=runner.split('#@$&') # Split the fields\n",
    "                        if len(runner2)==1:\n",
    "                            continue\n",
    "\n",
    "                        # It works ONLY if the number of fields are the same for different languages\n",
    "                        if len(runner2)<4:\n",
    "#                             print('Insufficient data to parse:',runner2)\n",
    "                            continue\n",
    "                        try:\n",
    "                            runner=process_fields2(runner2,pace=pace) \n",
    "                        except:\n",
    "                            error_links.append((i,link))\n",
    "                            continue\n",
    "                        if len(runner)==9:\n",
    "                            runner.append(link)\n",
    "                            final_list.append(runner)         # Append to the final list \n",
    "                            added_runner+=1\n",
    "                        else:\n",
    "                            try:\n",
    "                                if len(runner)>1:\n",
    "                                    print('Strange output:',added_runner)\n",
    "                                elif runner[0]=='Bad rank':\n",
    "                                    pass\n",
    "                                elif runner[0]=='pace not present':\n",
    "                                    pass\n",
    "#                                     print('No pace:',runner2)\n",
    "                                elif runner[0]=='Missing name':\n",
    "                                    print('No name:',runner2)\n",
    "                                elif runner[0]=='No year':\n",
    "                                    print('No year:',runner2)\n",
    "                                else:\n",
    "#                                     print(\"Bad PF:\",runner2)\n",
    "                                    pass\n",
    "                            except:\n",
    "                                print(runner)\n",
    "                                raise()\n",
    "        \n",
    "    print('Added runners =',added_runner)\n",
    "print('Time: ',time.time()-t1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(final_list)\n",
    "df2 = df2.rename(columns={0:'cat',1:'sex',2:'rang',3:'nom',4:'an',5:'lieu',6:'temps',7:'retard',8:'pace',9:'link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(df2.pace==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.link.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.to_csv('../main_database_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/stefano/Dropbox/Universita/Losanna/Exam/Anno2/DataAnalysis/main_database_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M40         39743\n",
       "M20         30235\n",
       "M50         24055\n",
       "M30         22914\n",
       "C           21180\n",
       "B           16715\n",
       "M45         16259\n",
       "HS2         15906\n",
       "HS1         14860\n",
       "13          14827\n",
       "D           14758\n",
       "F/3         11958\n",
       "12          11155\n",
       "E           10758\n",
       "W20         10689\n",
       "W40         10532\n",
       "H           10183\n",
       "Hommes3     10139\n",
       "Hommes2      9985\n",
       "15           9512\n",
       "C/M20        9399\n",
       "11           9253\n",
       "14           8912\n",
       "W35          8905\n",
       "04           8807\n",
       "A            8788\n",
       "F40          8582\n",
       "W30          8504\n",
       "M60          8064\n",
       "H/M35        7666\n",
       "            ...  \n",
       "MW65            1\n",
       "TMix            1\n",
       "H-W70           1\n",
       "RD50            1\n",
       "8km/H3          1\n",
       "Sen-W70+        1\n",
       "Fun             1\n",
       "5k/M60          1\n",
       "V/W55           1\n",
       "25k/W18         1\n",
       "JugAF           1\n",
       "MHL/F70         1\n",
       "TS2/W70         1\n",
       "B-D12           1\n",
       "21-F-Jun        1\n",
       "MW19            1\n",
       "WalkH           1\n",
       "CFH3            1\n",
       "W15             1\n",
       "8km/H2          1\n",
       "15/15/F1        1\n",
       "SB-10           1\n",
       "8km/D3          1\n",
       "CFD4            1\n",
       "FMA             1\n",
       "MTBDJ           1\n",
       "MF65            1\n",
       "TS3/W70         1\n",
       "H/W70           1\n",
       "MMFE            1\n",
       "Name: cat, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cat.value_counts()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
