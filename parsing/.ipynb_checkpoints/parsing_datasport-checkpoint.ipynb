{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Data from [datasport.com](https://www.datasport.com/en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use postman to understand the parameters used by the url request, asked for the exercise.\n",
    "\n",
    "(However, notice that there are equivalent tools for other browser - for instance, for firefox:\n",
    "http://stackoverflow.com/questions/28997326/postman-addons-like-in-firefox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important modules for this HW\n",
    "import bs4 # doc: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "import requests as rq \n",
    "\n",
    "\n",
    "# previous useful modules\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "form_source = rq.get(\"https://www.datasport.com/en/\")\n",
    "form_soup = bs4.BeautifulSoup(form_source.text, \"html.parser\")\n",
    "# print(form_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the `select` menus of the page, using the `find_all` method of *BeautifulSoup* which allows to search for all tags of a certain type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectors = form_soup.find_all('select')\n",
    "print(len(selectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, we can find out what each tag is about by printing the its `name` attribute :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for num, s in enumerate(selectors):\n",
    "    print(\"Select n°{} : {}\".format(num, s.attrs['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in selectors:\n",
    "    options = s.find_all('option')\n",
    "    options_desc_values = [(o.text, o.attrs['value']) for o in options]\n",
    "    print(s.attrs['name'] + ':')\n",
    "    for (d,v) in options_desc_values:\n",
    "        print(\"- {} [{}]\".format(d,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 parsed.\n",
      "2000 parsed.\n",
      "2001 parsed.\n",
      "2002 parsed.\n",
      "2003 parsed.\n",
      "2004 parsed.\n",
      "2005 parsed.\n",
      "2006 parsed.\n",
      "2007 parsed.\n",
      "2008 parsed.\n",
      "2009 parsed.\n",
      "2010 parsed.\n",
      "2011 parsed.\n",
      "2012 parsed.\n",
      "2013 parsed.\n",
      "2014 parsed.\n",
      "2015 parsed.\n"
     ]
    }
   ],
   "source": [
    "list_url = []\n",
    "list_dates = []\n",
    "list_names = []\n",
    "list_places = []\n",
    "\n",
    "etyp = 'Running'\n",
    "eventlocation = 'CCH'\n",
    "eventservice = 'all'\n",
    "\n",
    "eventmonth = []\n",
    "for month in range(12):\n",
    "    eventmonth.append(str(month+1).zfill(2))\n",
    "eventyear = []\n",
    "for year in range(1999,2016):\n",
    "    eventyear.append(str(year).zfill(4))\n",
    "\n",
    "yes_date = 0\n",
    "yes_rank = 0\n",
    "yes_name = 0\n",
    "yes_place = 0\n",
    "\n",
    "for year in eventyear:\n",
    "    print(year, 'parsed.')\n",
    "        \n",
    "    for month in eventmonth:\n",
    "        d = {'etyp': etyp, 'eventlocation': eventlocation, \n",
    "             'eventmonth': month, 'eventservice': eventservice,\n",
    "             'eventyear': year}\n",
    "        post_source = rq.post('https://www.datasport.com/fr/calendrier/',data=d)\n",
    "        form = bs4.BeautifulSoup(post_source.text, \"html.parser\")\n",
    "        #print(form.prettify())\n",
    "        \n",
    "        find_tr = form.find_all('tr')\n",
    "        for tr in find_tr:\n",
    "            if (tr.has_attr('class') and (tr['class'][0]=='even' \n",
    "                                          or tr['class'][0]=='odd')):\n",
    "                all_td = tr.find_all('td')\n",
    "                \n",
    "                find_a = all_td[4].find_all('a')\n",
    "                for a in find_a:\n",
    "                    if (a['href'].startswith('http://services.datasport.com/')\n",
    "                        and not a['href'].endswith('.pdf') \n",
    "                        and not a['href'].endswith('pavees')\n",
    "                        and not a['href'].endswith('mmc')):\n",
    "                        list_url.append(a['href'])\n",
    "                        yes_rank += 1\n",
    "                find_a2 = all_td[14].find_all('a')\n",
    "                \n",
    "                if(yes_rank > 0):\n",
    "                    for a2 in find_a2:\n",
    "                        if (a2['href'].startswith('http://maps.google.ch/')):\n",
    "                            list_places.append(a2['href'].split('=')[-1].split(',')[0])\n",
    "                            yes_place += 1\n",
    "\n",
    "                    find_a = all_td[1].find_all('a')\n",
    "                    list_names.append(find_a[0].contents[0])\n",
    "                    yes_name += 1\n",
    "                \n",
    "                    find_date = all_td[0].find_all('span')\n",
    "                    for date in find_date:\n",
    "                        if (date.has_attr('class') and date['class'][0]==''):\n",
    "                            the_date = date.contents[0]\n",
    "                            if the_date[-1]=='+':\n",
    "                                the_date = the_date[:-1]\n",
    "                            if the_date[-4:]==' bis':\n",
    "                                the_date = the_date[:-4]\n",
    "                            list_dates.append(date.contents[0])\n",
    "                            yes_date += 1\n",
    "                \n",
    "                #debugging step\n",
    "                if yes_date != yes_name or yes_date!=yes_rank or yes_date!= yes_place: \n",
    "                    print(yes_rank, yes_place, month, year)\n",
    "                    print(list_url[-1])\n",
    "                    print(list_dates[-1])\n",
    "                yes_rank = 0\n",
    "                yes_name = 0\n",
    "                yes_date = 0\n",
    "                yes_place = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(list_url)==len(list_names))\n",
    "print(len(list_names)==len(list_dates))\n",
    "print(len(list_dates)==len(list_places))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_runs_df = pd.DataFrame({ 'Name' : list_names,\n",
    "                    'Date' : list_dates,\n",
    "                    'Place' : list_places,\n",
    "                    'URL' : list_url })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Place</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sam. 27.03.1999</td>\n",
       "      <td>Männedörfler Waldlauf</td>\n",
       "      <td>Männedorf</td>\n",
       "      <td>http://services.datasport.com/1999/zkb/maennedorf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sam. 20.03.1999</td>\n",
       "      <td>Kerzerslauf</td>\n",
       "      <td>Kerzers</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/kerzers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>Luzerner Stadtlauf</td>\n",
       "      <td>Luzern</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/luzern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>20km de Lausanne</td>\n",
       "      <td>Lausanne</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/km20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>Chäsitzerlouf, Kehrsatz</td>\n",
       "      <td>Kehrsatz</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/kehrsatz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                     Name      Place  \\\n",
       "0  sam. 27.03.1999    Männedörfler Waldlauf  Männedorf   \n",
       "1  sam. 20.03.1999              Kerzerslauf    Kerzers   \n",
       "2  sam. 24.04.1999       Luzerner Stadtlauf     Luzern   \n",
       "3  sam. 24.04.1999         20km de Lausanne   Lausanne   \n",
       "4  sam. 24.04.1999  Chäsitzerlouf, Kehrsatz   Kehrsatz   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://services.datasport.com/1999/zkb/maennedorf  \n",
       "1    http://services.datasport.com/1999/lauf/kerzers  \n",
       "2     http://services.datasport.com/1999/lauf/luzern  \n",
       "3       http://services.datasport.com/1999/lauf/km20  \n",
       "4   http://services.datasport.com/1999/lauf/kehrsatz  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_runs_df.to_csv('links2runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some data\n",
    "\n",
    "In order to get started, we can now start collecting the results from the Lausanne marathone, one of the main early event in Switzerland.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the html of the main page, and __extract the relevant parameters__ to query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "laus_mar_url = 'https://services.datasport.com/2016/lauf/lamara/'\n",
    "result_html = rq.get(laus_mar_url)\n",
    "\n",
    "# use BS to get the classes in which the data is devided:\n",
    "\n",
    "result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "result_font = result_soup.find_all('font')\n",
    "\n",
    "print('number of categories in the main page:', len(result_font))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we look for the ones containing \n",
    "# '*** Overall ***', as they are the most general categories \n",
    "\n",
    "# this is indeed probably a GENERAL KEYWORD, as it's indeed found also in\n",
    "# events in other laungauges, \n",
    "# like https://services.datasport.com/2016/lauf/ascona-locarno-marathon/\n",
    "\n",
    "good_fonts_num = []\n",
    "\n",
    "for n_font, font in enumerate(result_font):\n",
    "    \n",
    "    if 'Overall' in font.findChild().get_text():\n",
    "            \n",
    "        good_fonts_num.append(n_font)\n",
    "        print(font.findChild().get_text())\n",
    "        \n",
    "        \n",
    "good_fonts_num = np.asarray(good_fonts_num)        \n",
    "        \n",
    "#  S***** -.- THERE IS A PROBLEM with the marathon hommes : \n",
    "# they are not in the same 'html shape' .. -.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_fonts_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we have to get all: href=RANG*** b\n",
    "\n",
    "rang_to_query = []\n",
    "\n",
    "for i in range(len(good_fonts_num)-1):\n",
    "        \n",
    "    my_font = result_font[good_fonts_num[i] + 1]\n",
    "    a_tag = my_font.find_all('a')\n",
    "    \n",
    "    for t in a_tag:\n",
    "    \n",
    "        if 'RANG' in t['href']:\n",
    "            \n",
    "            rang_to_query.append(t['href'])\n",
    "        \n",
    "#             print(t['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the datasport.com with the right parameters and finally get the __tables__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = \"https://services.datasport.com/2016/lauf/lamara\"\n",
    "base_url + '/' + rang_to_query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get raw HTML response\n",
    "result_html = rq.get(base_url, params=rang_to_query[0])\n",
    "\n",
    "# Use BeautifulSoup and extract the first (and only) HTML table\n",
    "result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "# result_table = result_soup.find_all('table')[0]\n",
    "\n",
    "# print(result_table.prettify())\n",
    "\n",
    "df_trail = pd.read_html(result_soup.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_trail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ******* ******* ******* ******* *******  \n",
    "# OLD CODE \n",
    "# ******* ******* ******* ******* ******* ******* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_html(result_table.decode())[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = df.loc[1]                # use row 2 as column names\n",
    "df = df.drop([0, 1])                  # drop useless first rows\n",
    "df = df.drop([np.nan], axis=1)        # drop useless nan column\n",
    "df.index = df['No Sciper']            # use sciper column as index\n",
    "\n",
    "# Drop some columns\n",
    "df = df.drop(['Orientation Bachelor', 'Orientation Master', 'Filière opt.', 'Type Echange', 'Ecole Echange'], axis=1)\n",
    "\n",
    "# Do some renaming\n",
    "df.index.name = 'sciper'\n",
    "df.columns = ['gender', 'full_name', 'specialization', 'minor', 'status', 'sciper']\n",
    "\n",
    "# Map gender to more standard names\n",
    "dict_gender = {'Monsieur': 'male','Madame': 'female'}\n",
    "df.gender.replace(dict_gender, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tools\n",
    "\n",
    "We can define a helper function which, given a base URL and a dictionary of parameters, will fetch the data and fill a DataFrame with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(base_url, params_dict):\n",
    "    \"\"\"Get data from IS-Academia in a pandas DataFrame\"\"\"\n",
    "    \n",
    "    # Same sequence of operations of above, with a check if the result_table is empty\n",
    "    \n",
    "    result_html = rq.get(base_url,params=params_dict)\n",
    "    result_soup = bs4.BeautifulSoup(result_html.text, \"lxml\")\n",
    "    result_table = result_soup.find_all('table')[0]\n",
    "    \n",
    "    if (result_table.text == ''):\n",
    "        # Return empty dataframe\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        # Build a DataFrame containing the data, with SCIPER as index\n",
    "        df = pd.read_html(result_table.decode())[0]\n",
    "        try:\n",
    "            df.columns = df.loc[1]                # use 2nd row as column names\n",
    "            df = df.drop([0, 1])                  # drop useless first rows\n",
    "            df = df.drop([np.nan], axis=1)        # drop useless nan column\n",
    "            df.index = df['No Sciper']            # use sciper column as index\n",
    "        \n",
    "            # Drop some columns\n",
    "            df = df.drop(['Orientation Bachelor', 'Orientation Master', 'Filière opt.', 'Type Echange', 'Ecole Echange'], axis=1)\n",
    "            # Do some renaming\n",
    "            df.index.name = 'sciper'\n",
    "            df.columns = ['gender', 'full_name', 'specialization', 'minor', 'status', 'sciper']\n",
    "            # Map gender to more standard names\n",
    "            dict_gender = {'Monsieur': 'male','Madame': 'female'}\n",
    "            df.gender.replace(dict_gender, inplace=True)\n",
    "        except:\n",
    "            df = pd.DataFrame()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines test this function with hardcoded values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = \"http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.html?\"\n",
    "params_dict = {\n",
    "    'ww_x_GPS': 2021043255,\n",
    "    'ww_i_reportModel': 133685247,\n",
    "    'ww_i_reportModelXsl': 133685270,\n",
    "    'ww_x_UNITE_ACAD': 249847,\n",
    "    'ww_x_PERIODE_ACAD': 355925344,\n",
    "    'ww_x_PERIODE_PEDAGO': 249108,\n",
    "    'ww_x_HIVERETE':2936286\n",
    "}\n",
    "\n",
    "get_data(base_url, params_dict).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's get all the possible values in a cleaner way and keep them in variables that we will use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acad_period = {}\n",
    "level = {}\n",
    "semester = {}\n",
    "acad_unit = {}\n",
    "\n",
    "for s in selectors:\n",
    "    options = s.find_all('option')\n",
    "    options_desc_values = [(o.text, o.attrs['value']) for o in options]\n",
    "    s_name = s.attrs['name']\n",
    "    choices = {d: int(v) for (d,v) in options_desc_values if d!=''}\n",
    "    \n",
    "    if s_name == 'ww_x_PERIODE_ACAD':\n",
    "        acad_period = choices\n",
    "    elif s_name == 'ww_x_PERIODE_PEDAGO':\n",
    "        level = choices\n",
    "    elif s_name == 'ww_x_HIVERETE':\n",
    "        for (d,v) in options_desc_values:\n",
    "            if 'automne' in d:\n",
    "                semester['automne'] = int(v)\n",
    "            elif 'printemps' in d:\n",
    "                semester['printemps'] =int(v)\n",
    "    elif s_name == 'ww_x_UNITE_ACAD':\n",
    "        acad_unit = choices\n",
    "\n",
    "# Example of result\n",
    "acad_period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get bachelor data for every year and store it if it's not empty\n",
    "import os\n",
    "local_dir = '.local-data'\n",
    "try:\n",
    "    os.mkdir(local_dir)\n",
    "except FileExistsError:\n",
    "    # directory exists\n",
    "    print(\"Using existing '\" + local_dir + \"' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fixed values\n",
    "params_dict = {\n",
    "    'ww_x_GPS': -1,\n",
    "    'ww_i_reportModel': 133685247,\n",
    "    'ww_i_reportModelXsl': 133685270,\n",
    "    'ww_x_UNITE_ACAD': acad_unit['Informatique']\n",
    "}\n",
    "\n",
    "# Iterate over all the varying params and keep only data for bachelors\n",
    "for year_key, year_value in acad_period.items():\n",
    "    for level_key, level_value in level.items():\n",
    "        for semester_key, semester_value in semester.items():\n",
    "            if 'bachelor' in level_key.lower():\n",
    "                params_dict['ww_x_PERIODE_ACAD'] = year_value\n",
    "                params_dict['ww_x_PERIODE_PEDAGO'] = level_value\n",
    "                params_dict['ww_x_HIVERETE'] = semester_value\n",
    "                \n",
    "                df = get_data(base_url, params_dict)\n",
    "                if not df.empty:\n",
    "                    # Persist dataframe locally with pickle\n",
    "                    filename = year_key + '-' + level_key.replace(' ', '-').lower() + '-' + semester_key\n",
    "                    df.to_pickle(local_dir + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the previous cell should download 60 files!, as you can check with this command:\n",
    "print(len([name for name in os.listdir(local_dir)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hereby show an example of dataframe laoded from the files previously download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_example = pd.read_pickle(local_dir + '/2007-2008-bachelor-semestre-6-printemps')\n",
    "df_example.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
