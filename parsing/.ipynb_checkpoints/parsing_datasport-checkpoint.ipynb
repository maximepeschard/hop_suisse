{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parsing of Data from [datasport.com](https://www.datasport.com/en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we want to extract the global information on all the competitions we want to analyse. We are interested in:\n",
    "* all the **running** races,\n",
    "* only in **Switzerland**. \n",
    "\n",
    "We have taken all the data between the first run present in *Datasport*, that is **from 1999, up to 2015**, not to have to deal with competitions that have not yet happened, or for which the data is not present yet.\n",
    "\n",
    "In this file, we want to extract the following information on the data:\n",
    "* **Date** of the race,\n",
    "* **Name** of the race,\n",
    "* **Place** where the race has been organised,\n",
    "* **URL** of the page where the rankings are given.\n",
    "\n",
    "To find such information, we use [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) and Postman to use the API present in the *Datasport* main page, opened in [*expert mode*](https://www.datasport.com/en/calendar/). Indeed the *expert mode* contains more information than the *simple mode*, and it contains more specifically all the information that we need. Since for each resquest through the API, we get back a maximum number of results per request and per page, we decide to make a request for each month and each year in the time interval we are interested in (from January 1999 to December 2015), in order to simplify the parsing. Indeed, all the running races organized each month can be presented in only one page.\n",
    "\n",
    "We understand how each of these pages' html have been written, and we extract the wanted data from it. Once all the data have been parsed and assembled into a pandas DataFrame, we load it to the file *links2runs.csv* so that we do not need to run this code more than once, since it takes some time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import required modules and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4 # doc: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "import requests as rq \n",
    "\n",
    "#%matplotlib inline\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we begin the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists that will contain the information we are looking for, per category (URL, \n",
    "# date, name, place)\n",
    "list_url = []\n",
    "list_dates = []\n",
    "list_names = []\n",
    "list_places = []\n",
    "\n",
    "# Fixed request parameters: Running races, in Switzerland, all type of running events.\n",
    "etyp = 'Running'\n",
    "eventlocation = 'CCH'\n",
    "eventservice = 'all'\n",
    "\n",
    "# Specify all the 12 months in number, and all the years between 1999 and 2016. We will\n",
    "# loop on those two lists, these are the variable request parameters. \n",
    "eventmonth = []\n",
    "for month in range(12):\n",
    "    eventmonth.append(str(month+1).zfill(2))\n",
    "eventyear = []\n",
    "for year in range(1999,2016):\n",
    "    eventyear.append(str(year).zfill(4))\n",
    "\n",
    "# Debugging parameters, to be sure that for each event, we have one date, one URL where \n",
    "# we can find the rankings, one name and one place. Not less, not more than that. \n",
    "yes_date = 0\n",
    "yes_rank = 0\n",
    "yes_name = 0\n",
    "yes_place = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 parsed.\n",
      "2000 parsed.\n",
      "2001 parsed.\n",
      "2002 parsed.\n",
      "2003 parsed.\n",
      "2004 parsed.\n",
      "2005 parsed.\n",
      "2006 parsed.\n",
      "2007 parsed.\n",
      "2008 parsed.\n",
      "2009 parsed.\n",
      "2010 parsed.\n",
      "2011 parsed.\n",
      "2012 parsed.\n",
      "2013 parsed.\n",
      "2014 parsed.\n",
      "2015 parsed.\n"
     ]
    }
   ],
   "source": [
    "# Loop on all the years and all the months of each year, and make a request.\n",
    "for year in eventyear:\n",
    "    for month in eventmonth:\n",
    "        d = {'etyp': etyp, 'eventlocation': eventlocation, \n",
    "             'eventmonth': month, 'eventservice': eventservice,\n",
    "             'eventyear': year}\n",
    "        post_source = rq.post('https://www.datasport.com/fr/calendrier/',data=d)\n",
    "        form = bs4.BeautifulSoup(post_source.text, \"html.parser\")\n",
    "        \n",
    "        # Each <tr> containing a 'class' attribute corresponds to an event, let us find \n",
    "        # all of them.\n",
    "        find_tr = form.find_all('tr')\n",
    "        for tr in find_tr:\n",
    "            if (tr.has_attr('class') and (tr['class'][0]=='even' \n",
    "                                          or tr['class'][0]=='odd')):\n",
    "                # For each event, the 5th <td> contains the URL where to find the \n",
    "                # rankings. This URL corresponds to the 'href' value of some <a>. \n",
    "                all_td = tr.find_all('td')\n",
    "                find_a = all_td[4].find_all('a')\n",
    "                for a in find_a:\n",
    "                    # Select only the 'href' of the <a> that contain the URLs than \n",
    "                    # interest us, i.e. the ones going to a datasport page. We also \n",
    "                    # handle three exceptions: we do not want any pdf (no parsing can be\n",
    "                    # done on them), nor the link about \"Course des pavÃ©es\" ('pavees') \n",
    "                    # that gives an error 404, nor the 'mmc' event that is in \n",
    "                    # Lichtenchtein and not in Switzerland (website error). \n",
    "                    if (a['href'].startswith('http://services.datasport.com/')\n",
    "                        and not a['href'].endswith('.pdf') \n",
    "                        and not a['href'].endswith('pavees')\n",
    "                        and not a['href'].endswith('mmc')):\n",
    "                        list_url.append(a['href'])\n",
    "                        yes_rank += 1\n",
    "                \n",
    "                if(yes_rank > 0):\n",
    "                    # If a ranking URL has been found, we can analyse the data. Let us\n",
    "                    # look for the other attributes. The 15th <td> of each event contains\n",
    "                    # the URL link to the place of the race on Google Maps. From it, we \n",
    "                    # extract the name of the city. \n",
    "                    find_a2 = all_td[14].find_all('a')\n",
    "                    for a2 in find_a2:\n",
    "                        if (a2['href'].startswith('http://maps.google.ch/')):\n",
    "                            list_places.append(a2['href'].split('=')[-1].split(',')[0])\n",
    "                            yes_place += 1\n",
    "                    \n",
    "                    # In the 2nd <td> of each event, in its first <a>, we extract the name\n",
    "                    # of the event. \n",
    "                    find_a = all_td[1].find_all('a')\n",
    "                    list_names.append(find_a[0].contents[0])\n",
    "                    yes_name += 1\n",
    "                    \n",
    "                    # In the 1st <td> of each event, in the <span> containing a void \n",
    "                    # attribute 'class', we extract the date of the event. If the event \n",
    "                    # is on many days (specified by a '+' or 'bis), we only keep the date \n",
    "                    # of the first day of the event.\n",
    "                    find_date = all_td[0].find_all('span')\n",
    "                    for date in find_date:\n",
    "                        if (date.has_attr('class') and date['class'][0]==''):\n",
    "                            the_date = date.contents[0]\n",
    "                            if the_date[-1]=='+':\n",
    "                                the_date = the_date[:-1]\n",
    "                            if the_date[-4:]==' bis':\n",
    "                                the_date = the_date[:-4]\n",
    "                            list_dates.append(date.contents[0])\n",
    "                            yes_date += 1\n",
    "                \n",
    "                # Debugging step\n",
    "                if yes_date != yes_name or yes_date!=yes_rank or yes_date!= yes_place: \n",
    "                    print(yes_rank, yes_place, month, year)\n",
    "                    print(list_url[-1])\n",
    "                    print(list_dates[-1])\n",
    "                yes_rank = 0\n",
    "                yes_name = 0\n",
    "                yes_date = 0\n",
    "                yes_place = 0\n",
    "    print(year, 'parsed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us double check that we have the same number of event and of each of the attributes we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: True\n"
     ]
    }
   ],
   "source": [
    "print('Answer:', len(list_url)==len(list_names) and\n",
    "      len(list_names)==len(list_dates) and\n",
    "      len(list_dates)==len(list_places))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we assemble all of these information into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_runs_df = pd.DataFrame({ 'Name' : list_names,\n",
    "                    'Date' : list_dates,\n",
    "                    'Place' : list_places,\n",
    "                    'URL' : list_url })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Place</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sam. 27.03.1999</td>\n",
       "      <td>MÃ¤nnedÃ¶rfler Waldlauf</td>\n",
       "      <td>MÃ¤nnedorf</td>\n",
       "      <td>http://services.datasport.com/1999/zkb/maennedorf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sam. 20.03.1999</td>\n",
       "      <td>Kerzerslauf</td>\n",
       "      <td>Kerzers</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/kerzers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>Luzerner Stadtlauf</td>\n",
       "      <td>Luzern</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/luzern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>20km de Lausanne</td>\n",
       "      <td>Lausanne</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/km20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sam. 24.04.1999</td>\n",
       "      <td>ChÃ¤sitzerlouf, Kehrsatz</td>\n",
       "      <td>Kehrsatz</td>\n",
       "      <td>http://services.datasport.com/1999/lauf/kehrsatz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                     Name      Place  \\\n",
       "0  sam. 27.03.1999    MÃ¤nnedÃ¶rfler Waldlauf  MÃ¤nnedorf   \n",
       "1  sam. 20.03.1999              Kerzerslauf    Kerzers   \n",
       "2  sam. 24.04.1999       Luzerner Stadtlauf     Luzern   \n",
       "3  sam. 24.04.1999         20km de Lausanne   Lausanne   \n",
       "4  sam. 24.04.1999  ChÃ¤sitzerlouf, Kehrsatz   Kehrsatz   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://services.datasport.com/1999/zkb/maennedorf  \n",
       "1    http://services.datasport.com/1999/lauf/kerzers  \n",
       "2     http://services.datasport.com/1999/lauf/luzern  \n",
       "3       http://services.datasport.com/1999/lauf/km20  \n",
       "4   http://services.datasport.com/1999/lauf/kehrsatz  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the DataFrame into a csv file in order to re-use this data without having to re-run all this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_runs_df.to_csv('links2runs.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
